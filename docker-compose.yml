services:
  frontend:
    build:
      context: ./app/frontend
      dockerfile: Dockerfile
    ports:
      - "3000:80"
    depends_on:
      - backend
    environment:
      - VITE_API_URL=http://localhost:8010

  backend:
    build:
      context: ./app/backend
      dockerfile: Dockerfile
    ports:
      - "8010:8010"
    environment:
      - HF_TOKEN=${HF_TOKEN}
      - WHISPER_MODEL=${WHISPER_MODEL:-large-v2}
      - WHISPER_DEVICE=${WHISPER_DEVICE:-cpu}
      - WHISPER_BATCH_SIZE=${WHISPER_BATCH_SIZE:-16}
      - WHISPER_LANGUAGE=${WHISPER_LANGUAGE:-de}
      - LLM_BASE_URL=${LLM_BASE_URL:-http://ollama:11434/v1}
      - LLM_MODEL=${LLM_MODEL:-qwen3:8b}
    volumes:
      - ./app/backend/uploads:/app/uploads
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8010/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 1200s  # 20 min grace period for model download & loading
    depends_on:
      - ollama

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    # To pull the model, run after starting:
    # docker compose exec ollama ollama pull qwen3:8b

volumes:
  ollama_data:
