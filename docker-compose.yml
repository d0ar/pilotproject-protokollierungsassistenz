# Docker Compose for Protokollierungsassistenz
# Uses pre-built images from GitHub Container Registry (ghcr.io)
# No HuggingFace token required - models are pre-bundled in the images!
#
# Usage: docker compose up -d
# GPU mode: docker compose -f docker-compose.yml -f docker-compose.gpu.yml up -d

services:
  frontend:
    image: ghcr.io/aihpi/pilotproject-protokollierungsassistenz/frontend:latest
    ports:
      - "3000:80"
    depends_on:
      - backend
    restart: unless-stopped

  backend:
    image: ghcr.io/aihpi/pilotproject-protokollierungsassistenz/backend:cpu-latest
    ports:
      - "8010:8010"
    environment:
      - WHISPER_MODEL=${WHISPER_MODEL:-large-v2}
      - WHISPER_DEVICE=${WHISPER_DEVICE:-cpu}
      - WHISPER_BATCH_SIZE=${WHISPER_BATCH_SIZE:-16}
      - WHISPER_LANGUAGE=${WHISPER_LANGUAGE:-de}
      - LLM_BASE_URL=${LLM_BASE_URL:-http://ollama:11434/v1}
      - LLM_MODEL=${LLM_MODEL:-qwen3:8b}
    volumes:
      - ./uploads:/app/uploads
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8010/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 120s # Reduced from 20 min - models are pre-loaded
    depends_on:
      - ollama
    restart: unless-stopped

  ollama:
    image: ollama/ollama:latest
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
      - ./scripts/ollama-entrypoint.sh:/ollama-entrypoint.sh:ro
    environment:
      - OLLAMA_MODEL=${LLM_MODEL:-qwen3:8b}
    entrypoint: ["/bin/bash", "/ollama-entrypoint.sh"]
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:11434/api/tags"]
      interval: 10s
      timeout: 5s
      retries: 60
      start_period: 300s # 5 min grace period for model download
    restart: unless-stopped

volumes:
  ollama_data:
